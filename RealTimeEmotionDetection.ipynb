{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3fd382b-84e0-4600-85f4-ba2c808a0d5a",
   "metadata": {},
   "source": [
    "## Import the required Libraries\n",
    "- `numpy as np`: Numerical computing library for array operations and mathematical functions.\n",
    "- `pandas as pd`: Data manipulation and analysis library, used for loading and handling datasets (e.g., CSV files).\n",
    "- `tensorflow as tf`: Machine learning framework for building and training neural networks, including the Keras API.\n",
    "- `tensorflow.keras.models`: Keras models module for creating sequential models and loading saved models.\n",
    "- `tensorflow.keras.layers`: Keras layers module for defining neural network layers such as Conv2D, MaxPooling2D, Flatten, Dense, Dropout, and BatchNormalization.\n",
    "- `tensorflow.keras.utils`: Keras utilities module, including to_categorical for converting labels to one-hot encoded format.\n",
    "- `sklearn.model_selection`: Scikit-learn module for splitting data into training and testing sets.\n",
    "- `cv2`: OpenCV library for image processing, such as loading and preprocessing images.\n",
    "- `matplotlib.pyplot as plt`: Matplotlib library for plotting and visualizing data.\n",
    "- `webbrowser`: Python module for opening URLs in a web browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f462b4fb-6a02-4e7a-88d9-abd952ba14ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import webbrowser\n",
    "\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaefe04-d6eb-4e7d-a5c0-7ed617726efd",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "This section outlines the preprocessing function for an emotion recognition model, handling missing data and preparing image and label data.\n",
    "\n",
    "## Emotion Classes\n",
    "- `emotion_classes`: List of emotion categories: Angry, Disgust, Fear, Happy, Sad, Surprise, Neutral.\n",
    "\n",
    "## Preprocessing Function\n",
    "- **Important Note**: Ensure all pixel data is properly formatted as strings and can be reshaped to 48x48x1 arrays to avoid runtime errors.\n",
    "- `preprocess(data)`: Function to preprocess the dataset.\n",
    "  - Checks for missing values and drops rows if found.\n",
    "  - Extracts and normalizes pixel data into 48x48x1 images.\n",
    "  - One-hot encodes emotion labels into 7 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bad38440-bcc9-45ed-ae10-0ee7e36130cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_classes = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "def preprocess(data):\n",
    "    # Check for missing values\n",
    "    if data.isnull().values.any():\n",
    "        print(\"Missing values found. Dropping rows with missing data...\")\n",
    "        data = data.dropna().reset_index(drop=True)\n",
    "    else:\n",
    "        print(\"No missing values found.\")\n",
    "\n",
    "    # Extract and normalize pixel data\n",
    "    pixels = data['pixels'].tolist()\n",
    "    images = np.array([\n",
    "        np.fromstring(pixel, sep=' ').reshape(48, 48, 1) for pixel in pixels\n",
    "    ])\n",
    "    images = images / 255.0  # Normalize\n",
    "\n",
    "    # One-hot encode emotion labels\n",
    "    labels = to_categorical(data['emotion'].values, num_classes=7)\n",
    "\n",
    "    return images, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700b85bc-8f71-4a52-bb2c-80c4279fc568",
   "metadata": {},
   "source": [
    "## Data Loading and Splitting\n",
    "- `data`: Loaded from `fer2013.csv` using pandas.\n",
    "- `train_data`: Subset where `Usage` is 'Training'.\n",
    "- `val_data`: Subset where `Usage` is 'PublicTest'.\n",
    "- `test_data`: Subset where `Usage` is 'PrivateTest'.\n",
    "\n",
    "## Preprocessing\n",
    "- `X_train, y_train`: Preprocessed training data and labels.\n",
    "- `X_val, y_val`: Preprocessed validation data and labels.\n",
    "- `X_test, y_test`: Preprocessed test data and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cf87020-465a-4e47-8a96-7a3ac26b1a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing values found.\n",
      "No missing values found.\n",
      "No missing values found.\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(r\"C:\\Users\\SWAROOP\\Desktop\\fer2013.csv\")\n",
    "\n",
    "train_data = data[data['Usage'] == 'Training']\n",
    "val_data = data[data['Usage'] == 'PublicTest']\n",
    "test_data = data[data['Usage'] == 'PrivateTest']\n",
    "\n",
    "X_train, y_train = preprocess(train_data)\n",
    "X_val, y_val = preprocess(val_data)\n",
    "X_test, y_test = preprocess(test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2e42da-0be7-4e56-ba5b-21b26fe848f5",
   "metadata": {},
   "source": [
    "# Building the Emotion Recognition Model\n",
    "\n",
    "This section defines the architecture and compilation of a Convolutional Neural Network (CNN) for emotion recognition.\n",
    "\n",
    "## Model Architecture\n",
    "- `build_model()`: Function to create the CNN model.\n",
    "\n",
    "### Input Layer\n",
    "- **Layer 1**: Conv2D with 64 filters, 3x3 kernel, ReLU activation, input shape (48, 48, 1).\n",
    "\n",
    "### Hidden Layers\n",
    "- **Layer 2**: BatchNormalization for stabilizing training.\n",
    "- **Layer 3**: MaxPooling2D with 2x2 pool size.\n",
    "- **Layer 4**: Conv2D with 128 filters, 3x3 kernel, ReLU activation.\n",
    "- **Layer 5**: BatchNormalization.\n",
    "- **Layer 6**: MaxPooling2D with 2x2 pool size.\n",
    "- **Layer 7**: Conv2D with 256 filters, 3x3 kernel, ReLU activation.\n",
    "- **Layer 8**: BatchNormalization.\n",
    "- **Layer 9**: MaxPooling2D with 2x2 pool size.\n",
    "- **Layer 10**: Flatten to convert 2D to 1D.\n",
    "- **Layer 11**: Dense with 512 units, ReLU activation.\n",
    "- **Layer 12**: Dropout with 0.5 rate to prevent overfitting.\n",
    "\n",
    "### Output Layer\n",
    "- **Layer 13**: Dense with 7 units, softmax activation for 7 emotion classes.\n",
    "\n",
    "## Model Compilation\n",
    "- `model.compile()`: Configures the model with:\n",
    "  - Optimizer: 'adam'.\n",
    "  - Loss function: 'categorical_crossentropy'.\n",
    "  - Metrics: 'accuracy'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40ca1e4d-f4bc-422b-bb7f-d8c803640f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SWAROOP\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "def build_model():\n",
    "    model = Sequential([\n",
    "        Conv2D(64, (3, 3), activation='relu', input_shape=(48, 48, 1)),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "        Conv2D(256, (3, 3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "        Flatten(),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(7, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8006c8de-9c78-41bc-8cc7-b75016931fcb",
   "metadata": {},
   "source": [
    "# Training and Evaluating\n",
    "\n",
    "This section covers the training, evaluation, and saving of the emotion recognition model.\n",
    "\n",
    "## Training the Model\n",
    "- `model.fit()`: Trains the model with:\n",
    "  - Training data: `X_train`, `y_train`.\n",
    "  - Epochs: 10.\n",
    "  - Batch size: 64.\n",
    "  - Validation data: `X_val`, `y_val`.\n",
    "\n",
    "## Evaluating the Model\n",
    "- `model.evaluate()`: Assesses the model on test data `X_test`, `y_test`.\n",
    "  - Returns test loss and test accuracy.\n",
    "  - Prints test accuracy with 2 decimal places.\n",
    "\n",
    "## Saving the Model\n",
    "- `model.save()`: Saves the trained model to 'emotion_recognition_model.h5'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1afdf000-cee0-42d6-8d92-a5ae1fa340e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 267ms/step - accuracy: 0.2795 - loss: 2.3486 - val_accuracy: 0.3031 - val_loss: 1.7117\n",
      "Epoch 2/10\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 259ms/step - accuracy: 0.4240 - loss: 1.4770 - val_accuracy: 0.3943 - val_loss: 1.5804\n",
      "Epoch 3/10\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 260ms/step - accuracy: 0.4769 - loss: 1.3684 - val_accuracy: 0.2435 - val_loss: 1.9236\n",
      "Epoch 4/10\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 258ms/step - accuracy: 0.5212 - loss: 1.2583 - val_accuracy: 0.4650 - val_loss: 1.3762\n",
      "Epoch 5/10\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 257ms/step - accuracy: 0.5463 - loss: 1.1899 - val_accuracy: 0.5300 - val_loss: 1.2476\n",
      "Epoch 6/10\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 262ms/step - accuracy: 0.5754 - loss: 1.1232 - val_accuracy: 0.4999 - val_loss: 1.3084\n",
      "Epoch 7/10\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 260ms/step - accuracy: 0.6127 - loss: 1.0338 - val_accuracy: 0.5439 - val_loss: 1.2241\n",
      "Epoch 8/10\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 262ms/step - accuracy: 0.6443 - loss: 0.9497 - val_accuracy: 0.4731 - val_loss: 1.4294\n",
      "Epoch 9/10\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 258ms/step - accuracy: 0.6646 - loss: 0.8773 - val_accuracy: 0.5612 - val_loss: 1.2049\n",
      "Epoch 10/10\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 258ms/step - accuracy: 0.7047 - loss: 0.7930 - val_accuracy: 0.5573 - val_loss: 1.2341\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 44ms/step - accuracy: 0.5815 - loss: 1.1538\n",
      "Test Accuracy: 0.57"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_val, y_val))\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_acc:.2f}\")\n",
    "\n",
    "model.save('emotion_recognition_model.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de52c4b5-ed39-45cd-b14b-22fee5c16a22",
   "metadata": {},
   "source": [
    "# Predicting Emotion from Image\n",
    "\n",
    "This section defines a function to predict emotions from a given image using the trained model.\n",
    "\n",
    "## Prediction Function\n",
    "- `predict_emotion_from_image(img_path, model)`: Predicts emotion from an image.\n",
    "  - Loads the image using `cv2.imread()`.\n",
    "  - Converts the image to grayscale with `cv2.cvtColor()`.\n",
    "  - Resizes the image to 48x48 with `cv2.resize()`.\n",
    "  - Reshapes and normalizes the image data.\n",
    "  - Uses `model.predict()` to get the prediction.\n",
    "  - Determines the emotion using `emotion_classes` and `np.argmax()`.\n",
    "  - Displays the image with the predicted emotion using `matplotlib`.\n",
    "  - Returns the predicted emotion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8340c86-9df7-4906-bd97-9428b7866cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emotion_from_image(img_path, model):\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        print(\"Error: Image not found.\")\n",
    "        return None\n",
    "    \n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    resized = cv2.resize(gray, (48, 48))\n",
    "    reshaped = resized.reshape(1, 48, 48, 1).astype('float32') / 255.0\n",
    "    pred = model.predict(reshaped)\n",
    "    emotion = emotion_classes[np.argmax(pred)]\n",
    "\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(img_rgb)\n",
    "    plt.title(f\"Predicted Emotion: {emotion}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    return emotion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3452c8df-6172-473a-8981-3c04d79f7210",
   "metadata": {},
   "source": [
    "# Predicting Emotion from Video Frame\n",
    "\n",
    "This section defines a function to predict emotions from a video frame using the trained model with face detection.\n",
    "\n",
    "## Prediction Function\n",
    "- `predict_emotion_from_frame(frame, model)`: Predicts emotion from a video frame.\n",
    "  - Converts the frame to grayscale with `cv2.cvtColor()`.\n",
    "  - Loads the Haar Cascade Classifier for face detection.\n",
    "  - Detects faces using `detectMultiScale()`.\n",
    "  - For each detected face:\n",
    "    - Extracts the region of interest (ROI).\n",
    "    - Resizes the ROI to 48x48 with `cv2.resize()`.\n",
    "    - Reshapes and normalizes the image data.\n",
    "    - Uses `model.predict()` to get the prediction.\n",
    "    - Determines the emotion using `emotion_classes` and `np.argmax()`.\n",
    "    - Draws a rectangle around the face and adds the emotion label with `cv2.rectangle()` and `cv2.putText()`.\n",
    "    - Returns the first detected emotion.\n",
    "  - Returns `None` if no faces are detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e75e4812-1469-4466-b218-7b0c94349b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emotion_from_frame(frame, model):\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        roi = gray[y:y+h, x:x+w]\n",
    "        resized = cv2.resize(roi, (48, 48))\n",
    "        reshaped = resized.reshape(1, 48, 48, 1).astype('float32') / 255.0\n",
    "        pred = model.predict(reshaped)\n",
    "        emotion = emotion_classes[np.argmax(pred)]\n",
    "\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "        cv2.putText(frame, emotion, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 255), 2)\n",
    "\n",
    "        return emotion\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebda8ec-6f7c-4820-88ae-6819542c9458",
   "metadata": {},
   "source": [
    "# Emotion-Based Recommendations\n",
    "\n",
    "This section defines a dictionary of recommendations and a function to provide tailored suggestions based on predicted emotions.\n",
    "\n",
    "## Recommendations Dictionary\n",
    "- `recommendations`: A dictionary mapping emotions to:\n",
    "  - `quote`: Inspirational quote.\n",
    "  - `song`: Suggested song with a YouTube URL.\n",
    "  - `url`: Link to the song on YouTube.\n",
    "  - `activity`: Suggested activity to enhance mood.\n",
    "\n",
    "### Emotion Categories\n",
    "- **Angry**: Quote by Ralph Waldo Emerson, song \"Calm Down\" by Taylor Swift, activity: deep breathing or walking.\n",
    "- **Disgust**: Quote about letting go, song \"Clean\" by Taylor Swift, activity: declutter or meditate.\n",
    "- **Fear**: Quote by Eleanor Roosevelt, song \"Brave\" by Sara Bareilles, activity: journal or talk to a friend.\n",
    "- **Happy**: Quote by Dalai Lama, song \"Happy\" by Pharrell Williams, activity: celebrate or share joy.\n",
    "- **Sad**: Quote by Robert H. Schuller, song \"Fix You\" by Coldplay, activity: watch a movie or call a loved one.\n",
    "- **Surprise**: Quote about embracing life, song \"Surprise Yourself\" by Jack Garratt, activity: try something new.\n",
    "- **Neutral**: Quote about doing nothing, song \"Let It Be\" by The Beatles, activity: take a break or enjoy tea.\n",
    "\n",
    "## Recommendation Function\n",
    "- `recommend_based_on_emotion(emotion)`: Provides recommendations based on the given emotion.\n",
    "  - Retrieves recommendation from the dictionary.\n",
    "  - Prints quote, song, and activity.\n",
    "  - Opens the song URL in a web browser.\n",
    "  - Prints \"No recommendation available\" if emotion is not found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa44da97-34c7-4a04-a74b-8c4168235176",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = {\n",
    "    'Angry': {\n",
    "        'quote': \"For every minute you remain angry, you give up sixty seconds of peace of mind. ‚Äì Ralph Waldo Emerson\",\n",
    "        'songs': {\n",
    "            'English': [\n",
    "                {\"title\": \"Calm Down ‚Äì Taylor Swift\", \"url\": \"https://www.youtube.com/watch?v=nfWlot6h_JM\"},\n",
    "                {\"title\": \"Let It Go ‚Äì Idina Menzel\", \"url\": \"https://www.youtube.com/watch?v=L0MK7qz13bU\"}\n",
    "            ],\n",
    "            'Telugu': [\n",
    "                {\"title\": \"Pranama ‚Äì Orange\", \"url\": \"https://www.youtube.com/watch?v=abVQHq2WuhU\"},\n",
    "                {\"title\": \"Manasa ‚Äì Munna\", \"url\": \"https://www.youtube.com/watch?v=kCJVR8X1NoY\"}\n",
    "            ]\n",
    "        },\n",
    "        'activity': \"Try deep breathing or go for a walk.\"\n",
    "    },\n",
    "    'Disgust': {\n",
    "        'quote': \"Let go of what doesn't serve you.\",\n",
    "        'songs': {\n",
    "            'English': [\n",
    "                {\"title\": \"Clean ‚Äì Taylor Swift\", \"url\": \"https://www.youtube.com/watch?v=WA4iX5D9Z64\"},\n",
    "                {\"title\": \"Demons ‚Äì Imagine Dragons\", \"url\": \"https://www.youtube.com/watch?v=mWRsgZuwf_8\"}\n",
    "            ],\n",
    "            'Telugu': [\n",
    "                {\"title\": \"Chal Chalo Chalo ‚Äì Happy Days\", \"url\": \"https://www.youtube.com/watch?v=4HDtPo4fK-0\"},\n",
    "                {\"title\": \"Em Cheppanu ‚Äì Oohalu Gusagusalade\", \"url\": \"https://www.youtube.com/watch?v=mnZc9F9NjF8\"}\n",
    "            ]\n",
    "        },\n",
    "        'activity': \"Do a quick declutter or meditate.\"\n",
    "    },\n",
    "    'Fear': {\n",
    "        'quote': \"Do one thing every day that scares you. ‚Äì Eleanor Roosevelt\",\n",
    "        'songs': {\n",
    "            'English': [\n",
    "                {\"title\": \"Brave ‚Äì Sara Bareilles\", \"url\": \"https://www.youtube.com/watch?v=QUQsqBqxoR4\"},\n",
    "                {\"title\": \"Fight Song ‚Äì Rachel Platten\", \"url\": \"https://www.youtube.com/watch?v=xo1VInw-SKc\"}\n",
    "            ],\n",
    "            'Telugu': [\n",
    "                {\"title\": \"Chakkani Bike Undi ‚Äì Ee Nagaraniki Emaindi\", \"url\": \"https://www.youtube.com/watch?v=izkKeHz7uRc\"},\n",
    "                {\"title\": \"Nuvvu Leka Nenu Lenu ‚Äì Gangotri\", \"url\": \"https://www.youtube.com/watch?v=IJCG8sDLpEo\"}\n",
    "            ]\n",
    "        },\n",
    "        'activity': \"Journal your thoughts or talk to a friend.\"\n",
    "    },\n",
    "    'Happy': {\n",
    "        'quote': \"Happiness is not something ready-made. It comes from your own actions. ‚Äì Dalai Lama\",\n",
    "        'songs': {\n",
    "            'English': [\n",
    "                {\"title\": \"Happy ‚Äì Pharrell Williams\", \"url\": \"https://www.youtube.com/watch?v=ZbZSe6N_BXs\"},\n",
    "                {\"title\": \"Best Day of My Life ‚Äì American Authors\", \"url\": \"https://www.youtube.com/watch?v=Y66j_BUCBMY\"}\n",
    "            ],\n",
    "            'Telugu': [\n",
    "                {\"title\": \"Oopiri Aaguthunnadey ‚Äì Tholi Prema\", \"url\": \"https://www.youtube.com/watch?v=3Ih04UotIBk\"},\n",
    "                {\"title\": \"Feel My Love ‚Äì Arya\", \"url\": \"https://www.youtube.com/watch?v=jH3Md3eAFw0\"}\n",
    "            ]\n",
    "        },\n",
    "        'activity': \"Celebrate the moment or share your joy!\"\n",
    "    },\n",
    "    'Sad': {\n",
    "        'quote': \"Tough times never last, but tough people do. ‚Äì Robert H. Schuller\",\n",
    "        'songs': {\n",
    "            'English': [\n",
    "                {\"title\": \"Fix You ‚Äì Coldplay\", \"url\": \"https://www.youtube.com/watch?v=k4V3Mo61fJM\"},\n",
    "                {\"title\": \"Someone Like You ‚Äì Adele\", \"url\": \"https://www.youtube.com/watch?v=hLQl3WQQoQ0\"}\n",
    "            ],\n",
    "            'Telugu': [\n",
    "                {\"title\": \"Kanulanu Thaake ‚Äì Geethanjali\", \"url\": \"https://www.youtube.com/watch?v=ZJP6bBoTbko\"},\n",
    "                {\"title\": \"Evare ‚Äì Premam\", \"url\": \"https://www.youtube.com/watch?v=nA0zHIVKwlA\"}\n",
    "            ]\n",
    "        },\n",
    "        'activity': \"Watch a feel-good movie or call a loved one.\"\n",
    "    },\n",
    "    'Surprise': {\n",
    "        'quote': \"Life is full of surprises. Embrace them!\",\n",
    "        'songs': {\n",
    "            'English': [\n",
    "                {\"title\": \"Surprise Yourself ‚Äì Jack Garratt\", \"url\": \"https://www.youtube.com/watch?v=5gHq6aY1bnY\"},\n",
    "                {\"title\": \"Pocketful of Sunshine ‚Äì Natasha Bedingfield\", \"url\": \"https://www.youtube.com/watch?v=yjuW7Hw4K3s\"}\n",
    "            ],\n",
    "            'Telugu': [\n",
    "                {\"title\": \"Yemito ‚Äì Ala Modalaindi\", \"url\": \"https://www.youtube.com/watch?v=g5cCJfvAc88\"},\n",
    "                {\"title\": \"Pilla ‚Äì Gabbar Singh\", \"url\": \"https://www.youtube.com/watch?v=8XzMzpP9R8Q\"}\n",
    "            ]\n",
    "        },\n",
    "        'activity': \"Try something new or spontaneous!\"\n",
    "    },\n",
    "    'Neutral': {\n",
    "        'quote': \"Sometimes doing nothing is doing something.\",\n",
    "        'songs': {\n",
    "            'English': [\n",
    "                {\"title\": \"Let It Be ‚Äì The Beatles\", \"url\": \"https://www.youtube.com/watch?v=QDYfEBY9NM4\"},\n",
    "                {\"title\": \"The Lazy Song ‚Äì Bruno Mars\", \"url\": \"https://www.youtube.com/watch?v=fLexgOxsZu0\"}\n",
    "            ],\n",
    "            'Telugu': [\n",
    "                {\"title\": \"Aaradugula Bullet ‚Äì Attarintiki Daredi\", \"url\": \"https://www.youtube.com/watch?v=yWhd3Jp_pKk\"},\n",
    "                {\"title\": \"Ee Manase ‚Äì Tholi Prema\", \"url\": \"https://www.youtube.com/watch?v=ffUJv1BoqZc\"}\n",
    "            ]\n",
    "        },\n",
    "        'activity': \"Take a break or enjoy a cup of tea.\"\n",
    "    }\n",
    "}\n",
    "\n",
    "import webbrowser\n",
    "\n",
    "def recommend_based_on_emotion(emotion):\n",
    "    rec = recommendations.get(emotion)\n",
    "    if rec:\n",
    "        print(f\"\\nüéß Recommendation for {emotion}:\")\n",
    "        print(f\"üí¨ Quote: {rec['quote']}\")\n",
    "        print(f\"üéØ Activity: {rec['activity']}\")\n",
    "\n",
    "        print(\"\\nüéµ English Playlist:\")\n",
    "        for idx, song in enumerate(rec['songs']['English'], 1):\n",
    "            print(f\"{idx}. {song['title']}\")\n",
    "\n",
    "        print(\"\\nüé∂ Telugu Playlist:\")\n",
    "        for idx, song in enumerate(rec['songs']['Telugu'], 1):\n",
    "            print(f\"{idx}. {song['title']}\")\n",
    "\n",
    "        # Optional: Let user choose a song to play\n",
    "        lang = input(\"\\nChoose playlist to open (English/Telugu): \").capitalize()\n",
    "        if lang in rec['songs']:\n",
    "            print(f\"\\nOpening first {lang} song: {rec['songs'][lang][0]['title']}\")\n",
    "            webbrowser.open(rec['songs'][lang][0]['url'])\n",
    "        else:\n",
    "            print(\"Invalid choice. No song played.\")\n",
    "    else:\n",
    "        print(\"No recommendation available.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b62fb4-07d1-4cf3-9bed-40efd46b3f88",
   "metadata": {},
   "source": [
    "# Emotion Detection Mode Selection\n",
    "\n",
    "This section implements a script to choose between image or live video input for emotion detection and provide recommendations.\n",
    "\n",
    "## Mode Selection and Execution\n",
    "- `input_mode`: User input to select mode ('live' or 'image').\n",
    "- `model`: Loads the trained model from 'emotion_recognition_model.h5'.\n",
    "\n",
    "### Image Mode\n",
    "- If `input_mode` is 'image':\n",
    "  - Prompts for an image path.\n",
    "  - Calls `predict_emotion_from_image()` to predict emotion.\n",
    "  - If successful, calls `recommend_based_on_emotion()` with the result.\n",
    "\n",
    "### Live Mode\n",
    "- If `input_mode` is 'live':\n",
    "  - Initializes video capture with `cv2.VideoCapture(0)`.\n",
    "  - Continuously reads frames and predicts emotions with `predict_emotion_from_frame()`.\n",
    "  - Displays the frame with detected emotions.\n",
    "  - Exits on 'q' key press, releases capture, and closes windows.\n",
    "  - If an emotion is detected, calls `recommend_based_on_emotion()`.\n",
    "\n",
    "### Invalid Mode\n",
    "- Prints \"Invalid mode selected.\" for unrecognized input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72498ba1-0660-4274-9245-5d42f1a393ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_mode = input(\"Enter input mode (live/image): \").lower()\n",
    "model = load_model('emotion_recognition_model.h5')\n",
    "\n",
    "if input_mode == \"image\":\n",
    "    path = input(\"Enter image path: \")\n",
    "    result = predict_emotion_from_image(path, model)\n",
    "    if result:\n",
    "        recommend_based_on_emotion(result)\n",
    "\n",
    "elif input_mode == \"live\":\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        result = predict_emotion_from_frame(frame, model)\n",
    "        cv2.imshow('Live Emotion Detection', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    if result:\n",
    "        recommend_based_on_emotion(result)\n",
    "\n",
    "else:\n",
    "    print(\"Invalid mode selected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63601380-25e0-4386-bfca-8715f901d5a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7fa7de-e7e3-4eb9-bc68-ec377b3eaddb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c073d0c-6b48-4b25-b600-89005755dcfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
